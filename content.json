{"pages":[{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"ASR论文阅读-RNNT群雄争霸","text":"RNNT近两年在语音识别领域异常火热，streaming transformer transducer更是被称为第4代语音识别系统。各厂论文不断，俨然称为兵家必争之地。本文首先介绍下RNNT，然后介绍一下相关的论文，最后说一下RNNT在OCR领域的方案验证。 RNNT简介RNNT在CTC的基础上发展而来（CTC和RNNT都是大神Graves搞出来的，真的强）。RNNT在CTC的基础上增加了pred和joint模块，可以将声学模型和语言模型结合起来。下面两张图来展示一下CTC和RNNT在网络结构上的异同 从上图中可以看出来，RNNT相比于CTC网络多了pred network和joint network。简单解释下，pred network其实就是一个语言模型（例如RNNLM），其输入是previous label，输出是标签的隐变量$p_{ui}$。joint network则是将声学模型和语言模型结合起来，最后进行转录。与CTC一样，RNNT也使用了特殊字符blank，但其对齐方式稍有不同。CTC的对齐要去重、去blank，RNNT则只需要去blank。它们的搜索方式也略有不同 CTC和RNNT都有blank，CTC想要跳到下一字符必须先过blank，一帧只能出一个字符或者多帧出一个字符（重复帧），而RNNT则可以在一帧里出很多不同的字符，等改帧信息消耗完了，才过blank。在路径的计算方式上，两者都可以使用动态规划进行求解。 主流模型比较的个人观点目前seq-to-seq模型主要有三种，CTC、RNNT（Tranducer）和LAS（基于注意力机制的ED模型），CTC广泛应用在ASR和OCR系统中，其效果也是相当好，同时小巧轻便，通常是实际部署中的最优选择。但CTC主要有三个不足之处， CTC的前提假设是每帧之间条件独立，在实际场景中，帧于帧之间肯定是有联系的。当然在实际中可以用RNN或者transformer模块将各帧之间联系起来。 CTC本身没有语言模型，不论是在ASR还是OCR领域，语言模型都是提升效果的法宝。而CTC本身不具有语言模型，词和词之间的依赖关系，只靠CTC是没办法解决了，当然可以通过外接语言模型进行后处理。 CTC的帧数必须大于等于标签数。基于CTC搜索方式的特性，CTC不具有一对多的能力，也就是说一帧特征只能有一个标签与其对应（多对一）。 RNNT作为CTC的扩展，当然就要解决上面的问题。pred network提供了语言模型，joint将声学模型和语言模型结合在一起。看起来RNNT的结构很完美。但其实RNNT也稍有不足。 joint network将两个模块的输出结合起来，假如标签空间是V，标签长度是U，帧长是T，那joint network为了找到最优路径，就得把所有路径都列举出来。也就是说要在TU的网格点上去搜索。UT*V呀，如果要识别的序列贼长，那网络肯定是承受不住的，而且这搜索空间这么大，网络可是不好训练的。 LAS模型。注意力机制，隐式语言模型。这些功能让LAS的识别效果是最好的。云端模型基本上都是采用这种结构了。当然LAS模型也是有缺点的。 注意力机制通常采用全局注意力，计算量是一个问题，但最大的问题是不能streaming。目前也有很多streaming注意力的方式。比如monotonic attention, monotonic chunkwise attention。这些后续再细读。 从实验结果上看(基于OCR和ASR的实验结果)， 效果上 CTC &lt; RNNT &lt; LAS 解码速度上 LAS &lt; RNNT &lt; CTC","link":"/2021/01/09/ASR%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RNNT%E7%BE%A4%E9%9B%84%E4%BA%89%E9%9C%B81/"},{"title":"OCR论文阅读-EP","text":"论文题目《Edit Probability for Scene Text Recognition》论文下载地址 文章简介       该论文发表于2018年，其主要是一种基于ED框架的场景文本识别方法。现有的方法基本优化的都是帧级别的最大似然损失。在训练过程中，当出现对齐混乱时，会使后面的帧都出现错误，这会增大训练代价。也就是说，优化帧级别的最大似然损失减少的是替换错误，而对于插入错误和删除错误，会产生额外的损失。举个例子，假设真实标签为”world”，网络前向预测为”wrld”，那么帧级别的最大似然损失就会有偏差。（这里的帧级别的概念是解码时一个帧代表一个字符。） 方法 编辑距离        编辑距离是常用的文本距离度量方法。其主要思想是一个文本经过多少次基本变换（移位，删除，替换）可以转化成里一个文本。本文将编辑距离引入网络训练中，减少对不齐带来的影响。 注意力机制解码器常用的注意力机制解码器公式如下 $$y_i=softmax(v^Ts_j),\\\\s_j=LSTM(y_j-1,c_j,s_j-1)\\\\c_j=\\sum_{k’^=1}^{|h|}\\alpha_{j,k}h_k,\\\\\\alpha_{j,k}=\\frac{exp(e_{j,k})}{\\sum_{k’=1}^{|h|}exp(e_{j,k})},\\\\e_{j,k}=w^Ttanh(Ws_{j-1}+Vh_k+b)$$其中$s_j$是解码器第j时刻的隐状态，$h_k$是编码器第k时刻的隐状态，$c_j$是编码器注意力加权平均后的context vector。       在这篇文章中，当解码第j个时刻时，解码器还会生成$R_j$和$I_j$，$$R_j=(R_j^C,R_j^I,R_j^D)^T=softmax(W_R^T,s_j),\\\\I_j=softmax(W_I^T,s_j)$$$R_j^C$, $R_j^I$, $R_j^D$分别代表$y_j$正确对齐的概率，缺少一个字符的概率，插入字符的概率。$I_j$是在$R_j^I$的条件下，缺失字符的概率分布。 编辑概率 结果 个人看法","link":"/2020/12/28/OCR%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-EP/"}],"tags":[{"name":"ASR","slug":"ASR","link":"/tags/ASR/"},{"name":"seq-to-seq","slug":"seq-to-seq","link":"/tags/seq-to-seq/"},{"name":"OCR","slug":"OCR","link":"/tags/OCR/"}],"categories":[{"name":"ASR","slug":"ASR","link":"/categories/ASR/"},{"name":"OCR识别","slug":"OCR识别","link":"/categories/OCR%E8%AF%86%E5%88%AB/"}]}